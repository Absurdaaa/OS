# Lab8 文件系统实验报告

## 一、实验目的

1. 了解基本的文件系统系统调用的实现方法
2. 了解一个基于索引节点组织方式的 Simple FS 文件系统的设计与实现
3. 了解文件系统抽象层 VFS 的设计与实现

## 二、实验内容

本次实验主要完成以下两个练习：

- 练习1：完成读文件操作的实现（`sfs_io_nolock` 函数）
- 练习2：完成基于文件系统的执行程序机制的实现（`load_icode` 函数）

---

## 三、练习1：完成读文件操作的实现

### 3.1 问题分析

`sfs_io_nolock` 函数的核心任务是在磁盘块与内存缓冲区之间传输文件数据。这个问题的难点在于：文件在磁盘上是按固定大小的块（4KB）存储的，但用户请求的读写范围 `[offset, offset + length)` 往往不会恰好与块边界对齐。

例如，用户想从文件的第 100 字节开始读取 5000 字节。假设块大小为 4096 字节，那么：
- 起始位置 100 落在第 0 块内，块内偏移为 100
- 结束位置 5100 落在第 1 块内，块内偏移为 1004
- 这个请求跨越了两个块，且首尾都不对齐

### 3.2 设计思路

针对上述问题，我将读写操作分为三个阶段处理：

**第一阶段：处理首块的不对齐部分**

如果 `offset` 不在块边界上（即 `offset % SFS_BLKSIZE != 0`），说明请求从某个块的中间开始。此时需要：
1. 通过 `sfs_bmap_load_nolock` 获取该逻辑块对应的物理块号
2. 使用 `sfs_buf_op`（部分块读写函数）读取从 `offset` 到该块末尾的数据
3. 更新已读取字节数和缓冲区指针

**第二阶段：处理中间的完整块**

对于完全落在请求范围内的整块，可以直接使用 `sfs_block_op`（整块读写函数）进行高效的整块传输。这里使用 while 循环逐块处理，每次读取一个完整的 4KB 块。

**第三阶段：处理尾块的不对齐部分**

如果结束位置不在块边界上，需要单独处理最后一个块。判断条件是 `alen < endpos - offset`，即已读取的字节数小于总请求长度，说明还有尾部数据需要处理。此时从最后一个块的起始位置读取剩余的字节数。

### 3.3 关键实现细节

**块号和偏移的计算**

代码中 `blkno = offset / SFS_BLKSIZE` 计算起始块的逻辑块号，`blkoff = offset % SFS_BLKSIZE` 计算块内偏移。`nblks` 表示中间完整跨过的块数，用于控制第二阶段的循环次数。

**读写操作的统一处理**

为了让同一套代码同时支持读和写操作，我使用了函数指针 `sfs_buf_op` 和 `sfs_block_op`。根据 `write` 参数的值，这两个指针分别指向读函数或写函数，从而避免了代码重复。

**边界条件的处理**

首块读取长度的计算需要特别注意：如果后面还有整块要处理（`nblks != 0`），则读到块末尾；否则直接读到 `endpos`。这个判断确保了当请求范围完全落在一个块内时也能正确处理。

### 3.4 与 SFS 文件系统的关系

`sfs_bmap_load_nolock` 函数是连接逻辑块号和物理块号的桥梁。SFS 文件系统使用 inode 结构管理文件，其中包含 12 个直接块指针和 1 个间接块指针。`sfs_bmap_load_nolock` 根据逻辑块索引，查找 inode 中对应的物理块号，必要时还会分配新块（用于写操作扩展文件）。

---

## 四、练习2：完成基于文件系统的执行程序机制的实现

### 4.1 问题分析

`load_icode` 函数负责将 ELF 格式的可执行文件加载到进程的内存空间中。与 Lab5 相比，Lab8 的主要变化是程序不再嵌入内核，而是存储在文件系统中，需要通过文件描述符读取。

这个函数需要完成的工作包括：创建进程的内存管理结构、解析 ELF 文件格式、将代码和数据加载到内存、建立用户栈、传递命令行参数、设置返回用户态的上下文。

### 4.2 ELF 文件加载过程

**读取和验证 ELF 头**

首先通过 `load_icode_read` 辅助函数从文件开头读取 ELF 头结构。这个辅助函数封装了文件定位（`sysfile_seek`）和读取（`sysfile_read`）操作。读取后检查魔数 `ELF_MAGIC` 以验证文件格式的合法性。

**解析程序头表**

ELF 头中的 `e_phoff` 字段指示程序头表的位置，`e_phnum` 指示程序头的数量。我通过循环遍历每个程序头，只处理类型为 `ELF_PT_LOAD` 的可加载段。

**建立虚拟内存映射**

对于每个可加载段，首先根据段的权限标志（`p_flags`）设置 VMA 的权限。ELF 中的 `PF_R`、`PF_W`、`PF_X` 分别对应读、写、执行权限，需要转换为页表项的 `PTE_R`、`PTE_W`、`PTE_X` 标志。然后调用 `mm_map` 在进程的地址空间中建立对应的虚拟内存区域。

**加载段内容到内存**

段的加载分为两部分：
1. TEXT/DATA 部分（`p_filesz` 字节）：从文件中读取实际内容，通过 `pgdir_alloc_page` 分配物理页，再用 `load_icode_read` 将数据读入
2. BSS 部分（`p_memsz - p_filesz` 字节）：不需要从文件读取，只需分配物理页并清零

这里的难点在于处理页边界对齐问题。段的起始地址可能不在页边界上，需要计算页内偏移，确保数据写入正确的位置。

### 4.3 用户栈的建立

用户栈位于用户地址空间的顶部（`USTACKTOP`），向下增长。我首先通过 `mm_map` 建立栈的 VMA，然后预分配 4 页物理内存作为初始栈空间。

命令行参数的传递是 Lab8 相比 Lab5 的新增内容。参数在栈上的布局如下（从高地址到低地址）：
1. 各个参数字符串的实际内容
2. `argv` 指针数组，每个元素指向对应的参数字符串
3. `argc` 的值

代码中先计算所有参数字符串的总长度，然后在栈顶下方预留空间���依次复制字符串并设置指针数组。

### 4.4 Trapframe 的设置

最后需要设置 trapframe，使得从内核返回时能正确进入用户程序的入口点。关键设置包括：
- `tf->gpr.sp`：用户栈指针，指向参数区域的底部
- `tf->gpr.a0` 和 `tf->gpr.a1`：分别传递 `argc` 和 `argv`，符合 RISC-V 调用约定
- `tf->epc`：程序入口地址，从 ELF 头的 `e_entry` 字段获取
- `tf->status`：清除 SPP 位（返回用户态）和 SIE 位（关闭内核中断），设置 SPIE 位（返回后开中断）

### 4.5 与 Lab5 的主要区别

Lab5 中程序以二进制数据的形式直接嵌入内核，通过指针访问；Lab8 中程序存储在文件系统，需要通过文件 I/O 接口读取。这要求我们：
1. 实现 `load_icode_read` 辅助函数封装文件读取操作
2. 在加载完成后关闭文件描述符
3. 正确处理 `argc`/`argv` 参数的传递

---

## 五、实验中的重要知识点

### 5.1 文件系统层次结构

本实验涉及的文件系统采用分层设计：

**VFS 层（虚拟文件系统）**：提供统一的文件操作接口（open、read、write、close 等），屏蔽底层文件系统的差异。用户程序和内核其他模块通过 VFS 接口访问文件，无需关心具体使用的是哪种文件系统。

**SFS 层（Simple File System）**：具体的文件系统实现。使用 inode 管理文件元数据，支持直接块和一级间接块的索引方式。`sfs_io_nolock` 就是 SFS 层的核心读写函数。

**设备层**：负责与底层块设备（如磁盘）交互，完成实际的数据传输。

这种分层设计体现了操作系统中"抽象"和"模块化"的设计思想，与 OS 原理中讲述的文件系统架构一致。

### 5.2 索引节点（inode）机制

SFS 使用 inode 存储文件的元数据，包括文件类型、大小、块指针等。每个文件对应一个 inode，通过 inode 号唯一标识。

inode 中的块指针分为直接块和间接块：
- 直接块：inode 中直接存储数据块的块号，访问速度快
- 间接块：inode 中存储一个索引块的块号，索引块中再存储数据块的块号，支持更大的文件

这种设计在 OS 原理中称为"多级索引"，是 Unix 文件系统的经典设计。

### 5.3 进程地址空间与 ELF 加载

`load_icode` 函数展示了进程地址空间的建立过程。一个用户进程的地址空间通常包括：
- 代码段（TEXT）：存放可执行指令，通常只读
- 数据段（DATA）：存放已初始化的全局变量
- BSS 段：存放未初始化的全局变量，加载时清零
- 堆（Heap）：动态内存分配区域
- 栈（Stack）：函数调用和局部变量

ELF 文件格式是 Unix/Linux 系统的标准可执行文件格式，程序头表描述了各个段的加载信息。这与 OS 原理中关于可执行文件格式和进程内存布局的内容相对应。

### 5.4 系统调用与用户态/内核态切换

`execve` 系统调用的实现涉及用户态和内核态的切换。用户程序通过 `ecall` 指令陷入内核，内核完成文件加载后，通过设置 trapframe 并执行 `sret` 指令返回用户态。

trapframe 中保存了返回用户态时需要恢复的寄存器状态，包括程序计数器（epc）、栈指针（sp）、状态寄存器（status）等。这体现了 OS 原理中关于中断/异常处理和上下文切换的知识。

---

## 六、OS 原理中重要但实验未涉及的知识点

**文件系统一致性与日志**：实际的文件系统需要考虑系统崩溃后的数据一致性问题。现代文件系统（如 ext4、NTFS）使用日志（journaling）技术记录操作，以便崩溃后恢复。本实验的 SFS 是简化实现，未涉及这方面内容。

**文件缓存（Buffer Cache）**：为了提高文件访问性能，操作系统通常在内存中缓存最近访问的磁盘块。本实验中每次读写都直接访问磁盘，未实现完整的缓存机制。

**磁盘调度算法**：当多个进程同时访问磁盘时，需要调度算法（如 SCAN、C-SCAN）优化磁头移动，减少寻道时间。本实验未涉及这部分内容。

**文件锁与并发控制**：多进程并发访问同一文件时需要同步机制。虽然代码中有 `lock_sin` 等锁操作，但实验未深入讨论文件锁的语义和实现。

**存储设备管理**：包括磁盘分区、RAID、逻辑卷管理等，这些在实际系统中很重要，但本实验使用的是简化的块设备模型。

---

## 七、实验结果

执行 `make qemu` 后，成功启动 shell 并能够执行各种测试程序：

- `softint`：正常退出，测试基本的程序加载和退出机制
- `faultread`：尝试读取地址 0，被系统杀死，显示 "error: -9 - process is killed"
- `faultreadkernel`：尝试读取内核地址，同样被杀死
- `testbss`：测试 BSS 段的正确初始化，输出 "testbss may pass." 后因越界写入被杀死
- `hello`：正常输出 "Hello world!!" 并退出

这些测试结果表明文件系统的读取功能和程序加载机制均已正确实现。

---

## 八、总结

通过本次实验，我深入理解了以下内容：

首先是文件系统的块级读写机制。`sfs_io_nolock` 函数的实现让我认识到，看似简单的文件读写操作，在底层需要处理块对齐、边界条件等诸多细节。分阶段处理不对齐块的思路是解决这类问题的通用方法。

其次是程序加载的完整流程。从打开 ELF 文件、解析文件头、建立内存映射、加载段内容、设置用户栈，到最后配置 trapframe 返回用户态，每一步都涉及操作系统的核心概念。这个过程将文件系统、内存管理、进程管理三个子系统有机地联系在一起。

最后是 VFS 抽象层的设计价值。通过 VFS 提供的统一接口，`load_icode` 可以使用相同的代码加载来自不同文件系统的程序，体现了良好的软件工程实践。

本实验是 uCore 系列实验的最后一个，它将前面实验中实现的各个子系统整合起来，形成了一个相对完整的操作系统。虽然与真实的操作系统相比还有很大差距，但通过这一系列实验，我对操作系统的核心原理有了更深刻的理解。
