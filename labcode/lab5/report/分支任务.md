# 实验报告：观察 `ecall` 和 `sret` 指令在 QEMU 中的处理流程

## 一、实验目的

1. 理解 RISC-V 特权指令 `ecall`（环境调用）和 `sret`（特权返回）的执行机制。
2. 学习 QEMU 对这些指令的处理流程，包括 TCG 翻译和 trap/异常处理。
3. 熟悉在 GDB 下单步调试内核和系统调用的技巧。

---

## 二、实验环境

* 操作系统：Ubuntu 22.04
* 仿真器：QEMU 6.2.0
* 编译工具链：riscv64-unknown-elf-gcc
* 调试工具：riscv64-unknown-elf-gdb（SiFive GDB-Metal 10.1）

---

## 三、实验过程

### 1. 双重gdb调试

我们首先需要修改Makefile中QEMU的路径，然后我们需要开三个终端，在第一个终端输入指令 

```gdb
make debug
```

然后在第二个终端中先输入指令

```gdb
pgrep -f qemu-system-riscv64
```

得到我们的PID为3516，接着输入指令 

```gdb
sudo gdb
```

在GDB中输入

```gdb
attach 3516
```

然后输入

```gdb
c
```

然后我们在第三个终端输入指令

```gdb
make gdb
```

然后输入分别输入

```gdb
add-symbol-file obj/__user_exit.out
```

然后我们分别在ecall以及sret的地方设置断点：

* 内核 trap 入口 `trapentry.S`
* 用户系统调用函数 `syscall.c`

```gdb
break user/libs/syscall.c:18
b /home/docay/OS/labcode/lab5/kern/trap/trapentry.S:133
```

然后我们使第二个断点失效，输入

```gdb
disable 2
```

接着输入c运行

---

### 2. 调试 `ecall` 指令

1. 单步执行到用户程序的 `syscall` 调用：

   ```gdb
   si
   ```
2. 当 PC 指向 `ecall` 指令时：

   ```gdb
   0x80008e <syscall+44>: ecall
   ```

   QEMU 会触发 trap，将控制权切换到内核异常处理入口 `__alltraps()`。
3. `__alltraps()` 的执行流程：

   * 调用 `SAVE_ALL` 保存所有通用寄存器和特权寄存器：

     ```asm
        0xffffffffc0200f18 <__alltraps+12>:  addi    sp,sp,-288
        0xffffffffc0200f1a <__alltraps+14>:  sd      zero,0(sp)
        0xffffffffc0200f1c <__alltraps+16>:  sd      ra,8(sp)
        0xffffffffc0200f1e <__alltraps+18>:  sd      gp,24(sp)
        0xffffffffc0200f20 <__alltraps+20>:  sd      tp,32(sp)
        0xffffffffc0200f22 <__alltraps+22>:  sd      t0,40(sp)
        0xffffffffc0200f24 <__alltraps+24>:  sd      t1,48(sp)
        0xffffffffc0200f26 <__alltraps+26>:  sd      t2,56(sp)
        0xffffffffc0200f28 <__alltraps+28>:  sd      s0,64(sp)
        0xffffffffc0200f2a <__alltraps+30>:  sd      s1,72(sp)
        0xffffffffc0200f2c <__alltraps+32>:  sd      a0,80(sp)
        0xffffffffc0200f2e <__alltraps+34>:  sd      a1,88(sp)
        0xffffffffc0200f30 <__alltraps+36>:  sd      a2,96(sp)
        0xffffffffc0200f32 <__alltraps+38>:  sd      a3,104(sp)
        0xffffffffc0200f34 <__alltraps+40>:  sd      a4,112(sp)
        0xffffffffc0200f36 <__alltraps+42>:  sd      a5,120(sp)
        0xffffffffc0200f38 <__alltraps+44>:  sd      a6,128(sp)
        0xffffffffc0200f3a <__alltraps+46>:  sd      a7,136(sp)
        0xffffffffc0200f3c <__alltraps+48>:  sd      s2,144(sp)
        0xffffffffc0200f3e <__alltraps+50>:  sd      s3,152(sp)
        0xffffffffc0200f40 <__alltraps+52>:  sd      s4,160(sp)
        0xffffffffc0200f42 <__alltraps+54>:  sd      s5,168(sp)
        0xffffffffc0200f44 <__alltraps+56>:  sd      s6,176(sp)
        0xffffffffc0200f46 <__alltraps+58>:  sd      s7,184(sp)
        0xffffffffc0200f48 <__alltraps+60>:  sd      s8,192(sp)
        0xffffffffc0200f4a <__alltraps+62>:  sd      s9,200(sp)
        0xffffffffc0200f4c <__alltraps+64>:  sd      s10,208(sp)
        0xffffffffc0200f4e <__alltraps+66>:  sd      s11,216(sp)
        0xffffffffc0200f50 <__alltraps+68>:  sd      t3,224(sp)
        0xffffffffc0200f52 <__alltraps+70>:  sd      t4,232(sp)
        0xffffffffc0200f54 <__alltraps+72>:  sd      t5,240(sp)
        0xffffffffc0200f56 <__alltraps+74>:  sd      t6,248(sp)
        0xffffffffc0200f58 <__alltraps+76>:  csrrw   s0,sscratch,zero
        0xffffffffc0200f5c <__alltraps+80>:  csrr    s1,sstatus
        0xffffffffc0200f60 <__alltraps+84>:  csrr    s2,sepc
        0xffffffffc0200f64 <__alltraps+88>:  csrr    s3,stval
        0xffffffffc0200f68 <__alltraps+92>:  csrr    s4,scause
        0xffffffffc0200f6c <__alltraps+96>:  sd      s0,16(sp)
        0xffffffffc0200f6e <__alltraps+98>:  sd      s1,256(sp)
        0xffffffffc0200f70 <__alltraps+100>: sd      s2,264(sp)
        0xffffffffc0200f72 <__alltraps+102>: sd      s3,272(sp)
        0xffffffffc0200f74 <__alltraps+104>: sd      s4,280(sp)
     ```
   * 将 trap frame 地址传递给 `trap()` 函数：

     ```asm
        0xffffffffc0200f76 <__alltraps+106>: mv      a0,sp
        0xffffffffc0200f78 <__alltraps+108>: jal     ra,0xffffffffc0200e80 <trap>
     ```
4. 内核 `trap()` 处理：

   * 调用 `trap_dispatch(tf)` 分发异常类型。
   * 对 `ecall`，最终调用 `syscall()` 处理具体系统调用。

5. QEMU处理：
   我们此时在第二个终端输入Ctrl+C，然后可以看到如下输出：
   ```gdb
   Thread 1 "qemu-system-ris" received signal SIGINT, Interrupt.
   0x00007a92251a6cb6 in __ppoll (fds=0x6387163d92f0, 
      nfds=6, timeout=<optimized out>, sigmask=0x0)
      at ../sysdeps/unix/sysv/linux/ppoll.c:44
   ```

   对于ppoll.c文件，默认的glibc二进制库并不包含源文件，所以GDB找不到。但是我们可以通过bt查看调用栈，我们接着在第二个终端输入

   ```gdb
   bt
   ```

   显示如下输出：

   ```gdb
   #0  0x00007a92251a6cb6 in __ppoll (
    fds=0x6387163d92f0, nfds=6, 
    timeout=<optimized out>, sigmask=0x0)
    at ../sysdeps/unix/sysv/linux/ppoll.c:44
   #1  0x00006386fc8aea3a in qemu_poll_ns (
      fds=0x6387163d92f0, nfds=6, timeout=1000000000)
      at util/qemu-timer.c:334
   #2  0x00006386fc8afc39 in os_host_main_loop_wait (
      timeout=1000000000) at util/main-loop.c:236
   #3  0x00006386fc8afd68 in main_loop_wait (
      nonblocking=0) at util/main-loop.c:517
   #4  0x00006386fc539dd8 in main_loop () at vl.c:1791
   #5  0x00006386fc5413d8 in main (argc=10, 
      argv=0x7ffd8d3f8a98, envp=0x7ffd8d3f8af0)
      at vl.c:4473
   ```

   我们接着将需要查看的变量值打印：

   ```gdb
   print *fds
   $1 = {fd = 0, events = 1, revents = 0}
   print nfds
   $2 = 6
   ```

   我们这里将展示除了main函数（这个函数太长了，接近2000行）以外的函数

   ```c
   static void main_loop(void)
   {
   #ifdef CONFIG_PROFILER
      int64_t ti;
   #endif
      while (!main_loop_should_exit()) {
   #ifdef CONFIG_PROFILER
         ti = profile_getclock();
   #endif
         main_loop_wait(false);
   #ifdef CONFIG_PROFILER
         dev_time += profile_getclock() - ti;
   #endif
      }
   }
   ```

   ```c
   void main_loop_wait(int nonblocking)
   {
      MainLoopPoll mlpoll = {
         .state = MAIN_LOOP_POLL_FILL,
         .timeout = UINT32_MAX,
         .pollfds = gpollfds,
      };
      int ret;
      int64_t timeout_ns;

      if (nonblocking) {
         mlpoll.timeout = 0;
      }

      /* poll any events */
      g_array_set_size(gpollfds, 0); /* reset for new iteration */
      /* XXX: separate device handlers from system ones */
      notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);

      if (mlpoll.timeout == UINT32_MAX) {
         timeout_ns = -1;
      } else {
         timeout_ns = (uint64_t)mlpoll.timeout * (int64_t)(SCALE_MS);
      }

      timeout_ns = qemu_soonest_timeout(timeout_ns,
                                       timerlistgroup_deadline_ns(
                                             &main_loop_tlg));

      ret = os_host_main_loop_wait(timeout_ns);
      mlpoll.state = ret < 0 ? MAIN_LOOP_POLL_ERR : MAIN_LOOP_POLL_OK;
      notifier_list_notify(&main_loop_poll_notifiers, &mlpoll);

      /* CPU thread can infinitely wait for event after
         missing the warp */
      qemu_start_warp_timer();
      qemu_clock_run_all_timers();
   }
   ```

   ```c
   static int os_host_main_loop_wait(int64_t timeout)
   {
      GMainContext *context = g_main_context_default();
      int ret;

      g_main_context_acquire(context);

      glib_pollfds_fill(&timeout);

      qemu_mutex_unlock_iothread();
      replay_mutex_unlock();

      ret = qemu_poll_ns((GPollFD *)gpollfds->data, gpollfds->len, timeout);

      replay_mutex_lock();
      qemu_mutex_lock_iothread();

      glib_pollfds_poll();

      g_main_context_release(context);

      return ret;
   }
   #else
   ```

   ```c
   /* qemu implementation of g_poll which uses a nanosecond timeout but is
   * otherwise identical to g_poll
   */
   int qemu_poll_ns(GPollFD *fds, guint nfds, int64_t timeout)
   {
   #ifdef CONFIG_PPOLL
      if (timeout < 0) {
         return ppoll((struct pollfd *)fds, nfds, NULL, NULL);
      } else {
         struct timespec ts;
         int64_t tvsec = timeout / 1000000000LL;
         /* Avoid possibly overflowing and specifying a negative number of
            * seconds, which would turn a very long timeout into a busy-wait.
            */
         if (tvsec > (int64_t)INT32_MAX) {
               tvsec = INT32_MAX;
         }
         ts.tv_sec = tvsec;
         ts.tv_nsec = timeout % 1000000000LL;
         return ppoll((struct pollfd *)fds, nfds, &ts, NULL);
      }
   #else
      return g_poll(fds, nfds, qemu_timeout_ns_to_ms(timeout));
   #endif
   }
   ```
6. QEMU 对 `ecall` 的处理

   通过上面的调用栈我们可以看到：

   ```gdb
   #0  __ppoll
   #1  qemu_poll_ns
   #2  os_host_main_loop_wait
   #3  main_loop_wait
   #4  main_loop
   #5  main
   ```

   这正是 **CPU 线程在主循环等待事件时的栈**。结合这个栈，我们可以分析 QEMU 是如何处理 guest `ecall` 的：

   i. **CPU 执行 `ecall`**

      * 在 TCG 或解释器模式下，QEMU 识别到 `ecall` 指令。
      * 调用内部函数（如 `cpu_trap(CPU_EXC_ECALL)`）触发 trap：

      * 设置寄存器 `mcause` 表示异常类型。
      * 保存返回地址 `mepc`。
      * 如有需要，设置 `mtval`/`stval`。

   ii. **CPU 线程可能阻塞等待事件**

      * 如果 `ecall` 模拟的系统调用需要等待 I/O 或定时器，CPU 线程会 **退出当前指令循环**。
      * 正如调用栈显示的：

      ```
      main_loop_wait → os_host_main_loop_wait → qemu_poll_ns → ppoll
      ```

      CPU 线程阻塞在 `ppoll()`，等待文件描述符或定时器事件。

   iii. **事件到达后处理 trap**

      * `qemu_poll_ns` 返回，主循环调用 `glib_pollfds_poll()` 处理就绪事件。
      * CPU 线程被唤醒，继续执行 `cpu_trap()` 或系统调用处理逻辑。
      * 更新寄存器状态，并准备返回执行下一条 guest 指令。

   iv. **执行完成，CPU 继续循环**

      * 处理完 `ecall` 相关逻辑后，CPU 线程重新进入 `cpu_exec_loop()`，继续执行后续指令。
      * 如果下一条指令是 `sret`，则直接恢复特权状态，返回用户态继续执行。

> **说明**：此过程展示了 QEMU 对 `ecall` 的关键处理——TCG 将 guest `ecall` 指令翻译为 host 代码块，触发 QEMU 内部 trap，然后进入内核处理。

---

### 3. 调试 `sret` 指令

1. 在内核处理完系统调用后，使用 `sret` 返回用户态：

   ```asm
   __trapret:
       ld s1, 256(sp)
       ld s2, 264(sp)
       ...
       sret
   ```
2. `sret` 会：

   * 恢复之前保存的特权寄存器状态（`sstatus`, `sepc`, `sscratch` 等）。
   * 切换回用户态 PC（`sepc`）。
   * 恢复寄存器值，实现上下文切换。
3. 单步观察到：

   * `__trapret` 恢复寄存器：

     ```asm
     ld ra, 8(sp)
     ld t0, 40(sp)
     ...
     csrw sstatus, s1
     csrw sepc, s2
     ```
   * 返回用户程序继续执行。

4. QEMU处理：
   我们先重启断点2，在第三个终端输入：

   ```gdb
   enable 2
   ```

   然后输入：

   ```gdb
   c
   ```

   然后我们在第二个终端中重复我们上面的操作，我们得到输出如下：

   ```gdb
   Thread 1 "qemu-system-ris" received signal SIGINT, Interrupt.
   0x00007a92251a6cb6 in __ppoll (fds=0x6387163d92f0, 
      nfds=6, timeout=<optimized out>, sigmask=0x0)
      at ../sysdeps/unix/sysv/linux/ppoll.c:44
   44      in ../sysdeps/unix/sysv/linux/ppoll.c
   (gdb) bt
   #0  0x00007a92251a6cb6 in __ppoll (
      fds=0x6387163d92f0, nfds=6, 
      timeout=<optimized out>, sigmask=0x0)
      at ../sysdeps/unix/sysv/linux/ppoll.c:44
   #1  0x00006386fc8aea3a in qemu_poll_ns (
      fds=0x6387163d92f0, nfds=6, timeout=1000000000)
      at util/qemu-timer.c:334
   #2  0x00006386fc8afc39 in os_host_main_loop_wait (
      timeout=1000000000) at util/main-loop.c:236
   #3  0x00006386fc8afd68 in main_loop_wait (
      nonblocking=0) at util/main-loop.c:517
   #4  0x00006386fc539dd8 in main_loop () at vl.c:1791
   #5  0x00006386fc5413d8 in main (argc=10, 
      argv=0x7ffd8d3f8a98, envp=0x7ffd8d3f8af0)
      at vl.c:4473
   (gdb) print *fds
   $3 = {fd = 0, events = 1, revents = 0}
   ```

5. QEMU 对 `sret` 的处理
   从上面的调用栈可以看到：

   ```gdb
   #0  __ppoll
   #1  qemu_poll_ns
   #2  os_host_main_loop_wait
   #3  main_loop_wait
   #4  main_loop
   #5  main
   ```

   这正是 **CPU 线程在主循环等待事件时的栈**，与 `ecall` 时类似。结合这个栈分析 QEMU 如何处理 guest `sret`：

   i. **CPU 执行 `sret`**

      * 指令恢复 Supervisor 模式下保存的寄存器状态：

      * `sstatus`：特权和中断状态
      * `sepc`：返回用户态的 PC
      * `sscratch` 等其他寄存器
      * 切换回用户态 PC，CPU 准备继续执行用户程序。

   ii. **CPU 线程可能阻塞等待事件**

      * 在 QEMU 中，即使是 `sret`，CPU 线程仍可能阻塞在主循环等待 I/O 或定时器事件。
      * 调用栈显示的：

      ```
      main_loop_wait → os_host_main_loop_wait → qemu_poll_ns → ppoll
      ```

      表示 CPU 线程当前阻塞在 `ppoll()`，等待主机事件。

   iii. **事件到达后继续执行**

      * 当 I/O 或定时器事件就绪：

      1. `qemu_poll_ns()` 返回。
      2. 主循环处理事件。
      3. CPU 线程恢复执行 `sret` 指令的剩余逻辑（已经恢复寄存器和 PC）。
      * CPU 线程继续进入 `cpu_exec_loop()`，执行用户程序下一条指令。

   iv. **栈与 `ecall` 相同的原因**

      * 调用栈显示的是 **CPU 线程的阻塞点**，与当前执行指令类型无关。
      * 无论是 `ecall`（触发 trap）还是 `sret`（恢复特权返回），CPU 线程都可能阻塞在同一个点等待主机事件。

> **说明**：
> `sret` 完成从 Supervisor 特权返回用户态，同时恢复寄存器状态。调用栈显示的阻塞点与 `ecall` 类似，是 CPU 线程在事件等待点阻塞的正常现象。

---

### 4. QEMU 处理机制分析

1. **TCG 指令翻译**：

   * QEMU 并不直接执行 guest 指令，而是动态将 guest 指令翻译为 host 指令块。
   * `ecall` / `sret` 会被翻译成 host 代码块，并在 host 上触发 trap/异常回调。
2. **Trap 处理流程**：

   ```
   Guest ecall -> TCG block -> host trap callback -> __alltraps() -> trap() -> trap_dispatch() -> syscall()
   ```
3. **寄存器保存与恢复**：

   * 内核通过 `SAVE_ALL` 保存通用寄存器。
   * 通过 `__trapret` 和 `sret` 恢复寄存器，实现从内核态返回用户态。

---

## 五、调试过程中观察到的寄存器变化

* `current->tf` 中记录了 trap frame：

  ```
  a0 = 30 (syscall号)
  a1 = 109
  sp = 0x7ffffe50
  pc = 0x800082 (syscall)
  status = 0x8020...
  ```
* `SAVE_ALL` 将 ra、t0-t6、s0-s11、a0-a7、sstatus、sepc、stval、scause 等寄存器保存到内核栈。
* `sret` 恢复时，将这些寄存器重新加载，实现用户态返回。

---

## 六、实验结论

1. `ecall` 指令在 QEMU 中触发 trap，由内核 `trap()` 和 `syscall()` 处理，整个过程通过 TCG 翻译实现 host 执行。
2. `sret` 指令完成从内核态返回用户态的上下文切换，恢复所有寄存器和特权状态。

# 抓马

其实这个后面的实验是在之前的版本上面跑的，不知道为啥交了个md之后代码不会在ecall断点停了（markdown，很神奇吧）

# 与大模型交互的心得

在一开始不知道怎么停在ecall指令处，在指导书中在si的时候可以看到每一步的汇编指令，但是实际的操作中显示0x0000000000001000 in ?? (),询问了ai才知道因为当前 PC 在物理地址 0x1000，那里没有加载任何符号（通常是 QEMU 的复位入口/boot ROM 区），gdb 自然显示 ??。后来通过设置断点在trapentry.S中成功停在了ecall指令处。

由于先按指导书里面的打点顺序，所以一直没有出现Continuing.的情况，询问ai之后，发现如果有断点就不会只显示Continuing.，而是会显示断点信息，所以在设置好断点之后再继续执行就可以看到Continuing.的信息了。